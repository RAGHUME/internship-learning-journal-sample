# OpenAI API, Embeddings, and Function Calling

This document covers practical implementations for interacting with the OpenAI API, building chatbots, handling image data, understanding vector embeddings, and enforcing structured outputs using function calling.

## 1. Interacting with the OpenAI API

### API Security and Setup
*   **Secure Storage:** It is a best practice to store your API keys in your system's `.bashrc` file rather than hardcoding them into Python scripts. 
*   **Environment Variables:** You can export the key by adding `export OPENAI_API_KEY="your_key"` to the `.bashrc` file, which prevents accidental exposure if you push your code to GitHub . In Python, you can access it securely using `os.getenv()` .

### Making HTTP Requests
*   **Libraries:** You can interact with the API using `curl` commands in the terminal or HTTP libraries like `requests` and `httpx` in Python .
*   **Request Structure:** A standard `POST` request requires three main components: a target URL, headers (containing the Authorization bearer token and Content-Type), and JSON data defining the model and messages .
*   **Handling Proxies:** If accessing the API through a proxy server (like an educational or organizational proxy), you must replace the default OpenAI base URL (`api.openai.com`) with the designated proxy URL.
*   **Parsing the Response:** The API returns a nested JSON object. To extract the actual text generated by the AI, you must navigate through the response dictionary, typically using `response['choices']['message']['content']`.

## 2. Building a Stateful Chatbot

*   **Stateless Nature of LLMs:** Large Language Models (LLMs) do not inherently have memory or remember past interactions. 
*   **Message History:** To build a chatbot with memory, you must maintain a list of all past messages (the context) and pass the entire list to the API with every new user query.
*   **Roles:** The message list is built using dictionaries containing `role` and `content` keys. There are three primary roles:
    *   **Developer/System:** High-level instructions that dictate how the model should behave, overriding other instructions (e.g., "answer within three words").
    *   **User:** The inputs, queries, or prompts provided by the human.
    *   **Assistant:** The replies generated by the AI model.

## 3. Image Handling with Base64 Encoding

*   **Converting Images to Text:** To send local images to an API, you must read the image file in binary mode (`rb`) and encode it into a Base64 format string using Python's `base64.b64encode` function. 
*   **Converting Text to Image:** A Base64 string can be converted back into binary data using `base64.b64decode` and written to a file to reconstruct the image.
*   **Lossless Process:** Base64 encoding and decoding is entirely lossless; the process involves no data compression, meaning the exact binary data is preserved perfectly.

## 4. Vector Embeddings & Semantic Similarity

*   **Semantic Representation:** Embeddings convert words or concepts into high-dimensional numerical vectors (often ranging from 500 to 1,500 dimensions). This allows computers to calculate the semantic meaning of words, understanding that "apple" and "fruit" are conceptually related.
*   **Distance Metrics:** 
    *   **Euclidean Distance:** Calculates the physical distance between two vectors. A **lower distance** indicates a closer semantic relationship.
    *   **Cosine Similarity:** Calculates the angle between two vectors using the dot product divided by the product of their magnitudes (norms). A **higher cosine similarity score** implies a smaller angle and greater semantic similarity.
*   **Text Embedding Models:** Dedicated embedding endpoints, like OpenAI's `text-embedding-3-small`, are highly cost-effective for converting text into vector arrays.

## 5. Multimodal Embeddings

*   **Dimensionality Constraint:** To calculate the distance or similarity between two embeddings, they must have the exact same number of dimensions . Standard text embedding models cannot be directly compared to image embedding models.
*   **Unified Models:** Multimodal embedding APIs (such as the Ginga API) can process different types of inputs (text, images, audio) and output them into vectors of identical dimensional length (e.g., 1024 dimensions). This allows for direct mathematical comparison between an image and a text string .

## 6. Function Calling for Structured Output

*   **Purpose:** Function calling forces the LLM to output its response in a strictly formatted JSON structure, which is critical for extracting specific data (like manufacturing dates or product names) from unstructured text or images.
*   **Implementation:** In the API request, alongside `model` and `messages`, you must pass a `tools` parameter containing the schema, and set `tool_choices` to "required" to force the model to utilize the specified tool.
*   **Defining the Schema:** The schema acts as a dictionary that explicitly defines the required properties, types (e.g., string, number), and descriptions of the data you want the AI to extract. 
*   **Retrieving the Data:** The generated JSON structure is nested differently than standard chat responses. It is typically found under `response['choices']['message']['tool_calls']['function']['arguments']`.